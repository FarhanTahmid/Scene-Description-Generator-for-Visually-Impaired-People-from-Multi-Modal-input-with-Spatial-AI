{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from scenedetect.video_manager import VideoManager\n",
    "from scenedetect.scene_manager import SceneManager\n",
    "from scenedetect.stats_manager import StatsManager\n",
    "from scenedetect.video_stream import VideoStream\n",
    "\n",
    "from scenedetect.detectors.content_detector import ContentDetector\n",
    "from scenedetect.detectors.threshold_detector import ThresholdDetector\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "BASE_DIR=os.environ['PROJECT_DIRECTORY']\n",
    "sys.path.append(BASE_DIR+'software_utils/')\n",
    "\n",
    "# To account for path errors\n",
    "try:\n",
    "    from models.image_captioner import ImageCaptioner\n",
    "    from models.video_captioner import VideoCaptioner\n",
    "    from models.encoderCNN import EncoderCNN\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from software_utils.create_transformer import create_transformer\n",
    "    from software_utils.vocabulary import Vocabulary\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=BASE_DIR\n",
    "coco_vocab_path=BASE_DIR+'Data/processed/coco_vocab.pkl'\n",
    "msrvtt_vocab_path='Data/processed/msrvtt_vocab.pkl'\n",
    "base_model='resnet152'\n",
    "ic_model_path='models/image_model/image_caption-model11-20-0.1309-5.0.pkl'\n",
    "vc_model_path='models/video_model/video_caption-model11-110-0.3354-5.0.pkl'\n",
    "im_embedding_size=2048\n",
    "vid_embedding_size=2048\n",
    "embed_size=256\n",
    "hidden_size=512\n",
    "num_frames=40\n",
    "max_caption_length=35\n",
    "ic_rnn_type='lstm'\n",
    "vc_rnn_type='gru'\n",
    "im_res=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(msrvtt_vocab_path, 'rb') as f:\n",
    "    msrvtt_vocab = pickle.load(f)\n",
    "with open(coco_vocab_path, 'rb') as f:\n",
    "    coco_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = create_transformer()\n",
    "encoder = EncoderCNN(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected RNN Type is lstm\n"
     ]
    }
   ],
   "source": [
    "video_captioner = VideoCaptioner(\n",
    "            vid_embedding_size,\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            len(msrvtt_vocab),\n",
    "            rnn_type='lstm',\n",
    "            start_id=msrvtt_vocab.word2idx[msrvtt_vocab.start_word],\n",
    "            end_id=msrvtt_vocab.word2idx[msrvtt_vocab.end_word]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\AppData\\Local\\Temp\\ipykernel_13504\\2863294166.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vc_checkpoint = torch.load(root_path + vc_model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is available\")\n",
    "    vc_checkpoint = torch.load(root_path + vc_model_path)\n",
    "else:\n",
    "    vc_checkpoint = torch.load(root_path + vc_model_path, map_location='cpu')\n",
    "video_captioner.load_state_dict(vc_checkpoint['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farha\\AppData\\Local\\Temp\\ipykernel_13504\\1358186514.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vc_checkpoint = torch.load(vc_model_path)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is available\")\n",
    "    vc_checkpoint = torch.load(vc_model_path)\n",
    "    encoder.cuda()\n",
    "    video_captioner.cuda()\n",
    "else:\n",
    "    vc_checkpoint = torch.load(vc_model_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoCaptioner(\n",
       "  (inp): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (inp_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (inp_bn): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (embed): Embedding(14748, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (out): Linear(in_features=512, out_features=14748, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_captioner.load_state_dict(vc_checkpoint['params'])\n",
    "encoder.eval()\n",
    "video_captioner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture live video and generate captions\n",
    "def live_scene_caption(camera_index: int = 0):\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(f\"Unable to access camera with index {camera_index}\")\n",
    "\n",
    "    # Initialize StatsManager and SceneManager\n",
    "    stats_manager = StatsManager()\n",
    "    scene_manager = SceneManager(stats_manager)\n",
    "    scene_manager.add_detector(ContentDetector(threshold=30, min_scene_len=40))\n",
    "\n",
    "    # Variables to track scene and embeddings\n",
    "    frame_count = 0\n",
    "    vid_array = None\n",
    "    scene_start_frame = 0\n",
    "    captured_frames = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convert frame to RGB format for PIL processing\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "            # Update the scene manager with the current frame\n",
    "            scene_manager._process_frame(frame_count, frame_rgb)\n",
    "\n",
    "            # Check if a new scene has been detected\n",
    "            scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "                        \n",
    "            # if scene_list and scene_list[-1][1].get_frames() == frame_count:\n",
    "            #     # Scene change detected, process the previous scene\n",
    "            #     if vid_array is not None:\n",
    "            #         # Encode the collected frames as a video embedding\n",
    "            #         vid_embeddings = encoder(vid_array)\n",
    "\n",
    "            #         # Predict caption for the detected scene\n",
    "            #         encoded_captions = video_captioner.predict(vid_embeddings.unsqueeze(0), beam_size=5).cpu().numpy().astype(int)\n",
    "            #         captions = [msrvtt_vocab.decode(caption, clean=True, join=True) for caption in encoded_captions]\n",
    "            #         print(captions)\n",
    "            #         # Print the generated caption for the scene\n",
    "            #         print(f\"Scene change detected at frame {frame_count}: {captions[0]}\")\n",
    "                \n",
    "            #     # Start capturing frames for the next scene\n",
    "            #     vid_array = torch.zeros((num_frames, 3, 224, 224))\n",
    "            #     captured_frames = []\n",
    "            #     scene_start_frame = frame_count\n",
    "                \n",
    "            # else:\n",
    "            if vid_array is not None:\n",
    "                # Encode the collected frames as a video embedding\n",
    "                vid_embeddings = encoder(vid_array)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    vid_embeddings = vid_embeddings.cuda()\n",
    "                \n",
    "                # Predict caption for the detected scene\n",
    "                encoded_captions = video_captioner.predict(vid_embeddings, beam_size=5).cpu().numpy().astype(int)\n",
    "                captions = [msrvtt_vocab.decode(caption, clean=True, join=True) for caption in encoded_captions]\n",
    "                # Overlay the description on the frame\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cv2.putText(frame, captions[0], (10, 50), font, 1, (255, 0, 255), 2, cv2.LINE_AA)\n",
    "                            \n",
    "            # Start capturing frames for the next scene\n",
    "            vid_array = torch.zeros((num_frames, 3, 224, 224))\n",
    "            vid_array=vid_array.cuda()\n",
    "            captured_frames = []\n",
    "            scene_start_frame = frame_count\n",
    "                \n",
    "            # Process frames for embedding if within num_frames\n",
    "            if frame_count - scene_start_frame < num_frames and vid_array is not None:\n",
    "                try:\n",
    "                    frame_pil = Image.fromarray(frame_rgb).convert('RGB')\n",
    "                    if torch.cuda.is_available():\n",
    "                        frame_tensor = transformer(frame_pil).cuda().unsqueeze(0)\n",
    "                    else:\n",
    "                        frame_tensor = transformer(frame_pil).unsqueeze(0)\n",
    "                        \n",
    "                    vid_array[frame_count - scene_start_frame] = frame_tensor\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame at index {frame_count}: {e}\")\n",
    "                    \n",
    "            # Display the video feed\n",
    "            cv2.imshow('Live Video Feed', frame)\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_scene_caption(camera_index=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
