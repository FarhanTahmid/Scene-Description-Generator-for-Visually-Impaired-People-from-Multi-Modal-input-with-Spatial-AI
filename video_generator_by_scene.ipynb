{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Video Generator By Scene</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sns1-node3/Farhan/Project-CSE499B/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import PIL\n",
    "import skimage.io as io\n",
    "import torch\n",
    "\n",
    "from scenedetect.video_manager import VideoManager\n",
    "from scenedetect.scene_manager import SceneManager\n",
    "from scenedetect.stats_manager import StatsManager\n",
    "\n",
    "from scenedetect.detectors.content_detector import ContentDetector\n",
    "from scenedetect.detectors.threshold_detector import ThresholdDetector\n",
    "from typing import List, Tuple\n",
    "\n",
    "import cv2\n",
    "\n",
    "BASE_DIR=os.environ['PROJECT_DIRECTORY']\n",
    "sys.path.append(BASE_DIR+'software_utils/')\n",
    "\n",
    "# To account for path errors\n",
    "try:\n",
    "    from models.image_captioner import ImageCaptioner\n",
    "    from models.video_captioner import VideoCaptioner\n",
    "    from models.encoderCNN import EncoderCNN\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from software_utils.create_transformer import create_transformer\n",
    "    from software_utils.vocabulary import Vocabulary\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=BASE_DIR\n",
    "coco_vocab_path=BASE_DIR+'Data/processed/coco_vocab.pkl'\n",
    "msrvtt_vocab_path='Data/processed/msrvtt_vocab.pkl'\n",
    "base_model='resnet152'\n",
    "ic_model_path='models/image_caption-model11-20-0.1309-5.0.pkl'\n",
    "vc_model_path='models/video_models/video_caption-model11-110-0.3354-5.0.pkl'\n",
    "im_embedding_size=2048\n",
    "vid_embedding_size=2048\n",
    "embed_size=256\n",
    "hidden_size=512\n",
    "num_frames=40\n",
    "max_caption_length=35\n",
    "ic_rnn_type='lstm'\n",
    "vc_rnn_type='gru'\n",
    "im_res=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(msrvtt_vocab_path, 'rb') as f:\n",
    "    msrvtt_vocab = pickle.load(f)\n",
    "with open(coco_vocab_path, 'rb') as f:\n",
    "    coco_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sns1-node3/Farhan/Project-CSE499B/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sns1-node3/Farhan/Project-CSE499B/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "transformer = create_transformer()\n",
    "encoder = EncoderCNN(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected RNN Type is lstm\n"
     ]
    }
   ],
   "source": [
    "video_captioner = VideoCaptioner(\n",
    "            vid_embedding_size,\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            len(msrvtt_vocab),\n",
    "            rnn_type='lstm',\n",
    "            start_id=msrvtt_vocab.word2idx[msrvtt_vocab.start_word],\n",
    "            end_id=msrvtt_vocab.word2idx[msrvtt_vocab.end_word]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2057398/1285781116.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vc_checkpoint = torch.load(root_path + vc_model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    vc_checkpoint = torch.load(root_path + vc_model_path)\n",
    "else:\n",
    "    vc_checkpoint = torch.load(root_path + vc_model_path, map_location='cpu')\n",
    "video_captioner.load_state_dict(vc_checkpoint['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoCaptioner(\n",
       "  (inp): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (inp_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (inp_bn): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (embed): Embedding(14748, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (out): Linear(in_features=512, out_features=14748, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    video_captioner.cuda()\n",
    "\n",
    "encoder.eval()\n",
    "video_captioner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scene_changes(video_path: str, method: str = 'threshold', new_stat_file: bool = True) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Detect scene changes in a given video.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to the video to analyze.\n",
    "        method: Method for detecting scene changes ('content' or 'threshold').\n",
    "        new_stat_file: Whether to create a new stats file.\n",
    "\n",
    "    Returns:\n",
    "        List of scene changes with their corresponding frame ranges as tuples.\n",
    "    \"\"\"\n",
    "    # Initialize the VideoManager and StatsManager.\n",
    "    video_manager = VideoManager([video_path])\n",
    "    stats_manager = StatsManager()\n",
    "\n",
    "    # Create a SceneManager and add the appropriate detector.\n",
    "    scene_manager = SceneManager(stats_manager)\n",
    "    if method == 'content':\n",
    "        scene_manager.add_detector(ContentDetector(threshold=30, min_scene_len=40))\n",
    "    else:\n",
    "        scene_manager.add_detector(ThresholdDetector(threshold=125, min_scene_len=40))\n",
    "\n",
    "    # Set the path for the stats file.\n",
    "    stats_file_path = f'{video_path}.{method}.stats.csv'\n",
    "    scene_list = []\n",
    "\n",
    "    try:\n",
    "        # Load stats file if it exists and new_stat_file is False.\n",
    "        if not new_stat_file and os.path.exists(stats_file_path):\n",
    "            with open(stats_file_path, 'r') as stats_file:\n",
    "                stats_manager.load_from_csv(stats_file)\n",
    "\n",
    "        # Set the downscale factor for faster processing.\n",
    "        video_manager.set_downscale_factor(2)\n",
    "\n",
    "        # Start the video manager.\n",
    "        video_manager.start()\n",
    "\n",
    "        # Perform scene detection.\n",
    "        scene_manager.detect_scenes(video_manager)\n",
    "\n",
    "        # Obtain the list of scenes.\n",
    "        scene_list = scene_manager.get_scene_list()\n",
    "        # Each scene is a tuple of (start_frame, end_frame).\n",
    "\n",
    "        # Save stats if required.\n",
    "        if stats_manager.is_save_required():\n",
    "            with open(stats_file_path, 'w') as stats_file:\n",
    "                stats_manager.save_to_csv(stats_file)\n",
    "\n",
    "    finally:\n",
    "        # Release the video manager resources.\n",
    "        video_manager.release()\n",
    "    # print(scene_list)\n",
    "    return scene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path=os.environ['PROJECT_DIRECTORY'] + 'Dataset/MSR-VTT/TrainVal/video66.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VideoManager is deprecated and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenes: [(00:00:00.000 [frame=0, fps=25.000], 00:00:02.080 [frame=52, fps=25.000]), (00:00:02.080 [frame=52, fps=25.000], 00:00:05.520 [frame=138, fps=25.000]), (00:00:05.520 [frame=138, fps=25.000], 00:00:07.200 [frame=180, fps=25.000]), (00:00:07.200 [frame=180, fps=25.000], 00:00:10.280 [frame=257, fps=25.000]), (00:00:10.280 [frame=257, fps=25.000], 00:00:11.000 [frame=275, fps=25.000])]\n",
      "Scene change timecode: [('00:00:00.000', '00:00:02.080'), ('00:00:02.080', '00:00:05.520'), ('00:00:05.520', '00:00:07.200'), ('00:00:07.200', '00:00:10.280'), ('00:00:10.280', '00:00:11.000')]\n",
      "Scene change idxs:[0, 52, 138, 180, 257]\n"
     ]
    }
   ],
   "source": [
    "scenes = find_scene_changes(video_path, method='content', new_stat_file=True)\n",
    "print(f'Scenes: {scenes}')\n",
    "scene_change_timecodes = [(scene[0].get_timecode(), scene[1].get_timecode())for scene in scenes]\n",
    "print(f\"Scene change timecode: {scene_change_timecodes}\")\n",
    "scene_change_idxs = [scene[0].get_frames() for scene in scenes]\n",
    "\n",
    "print(f\"Scene change idxs:{scene_change_idxs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene Change detected!\n"
     ]
    }
   ],
   "source": [
    "if len(scene_change_idxs) == 0:\n",
    "    print(\"No Scene Change!\")\n",
    "    scene_change_timecodes = ['00:00:00']\n",
    "    scene_change_idxs = [0]\n",
    "else:\n",
    "    print(\"Scene Change detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty torch tensor's to store values\n",
    "vid_embeddings = torch.zeros(\n",
    "    len(scene_change_idxs), num_frames, vid_embedding_size)\n",
    "if torch.cuda.is_available():\n",
    "    vid_embeddings = vid_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine last frame to analyze\n",
    "last_frame = scene_change_idxs[-1] + num_frames + 1\n",
    "\n",
    "frame_idx = 0\n",
    "cap_start_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and store relevant frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret or frame_idx == last_frame:\n",
    "        break\n",
    "\n",
    "    # Start storing frames\n",
    "    if frame_idx in scene_change_idxs:\n",
    "        cap_start_idx = frame_idx\n",
    "        vid_array = torch.zeros(num_frames, 3, 224, 224)\n",
    "\n",
    "    # Transform, and store\n",
    "    if frame_idx - cap_start_idx < num_frames:\n",
    "        try:\n",
    "            frame = PIL.Image.fromarray(frame).convert('RGB')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                frame = transformer(frame).cuda().unsqueeze(0)\n",
    "            else:\n",
    "                frame = transformer(frame).unsqueeze(0)\n",
    "\n",
    "            vid_array[frame_idx - cap_start_idx] = frame\n",
    "\n",
    "        except OSError as e:\n",
    "            print(e + \" could not process frame in \" + f)\n",
    "\n",
    "    # If at scene ending frame, encode the collected scene\n",
    "    if frame_idx - cap_start_idx == num_frames:\n",
    "        if torch.cuda.is_available():\n",
    "            vid_array = vid_array.cuda()\n",
    "        vid_embeddings[scene_change_idxs.index(\n",
    "            cap_start_idx)] =encoder(vid_array)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict captions using the video embeddings\n",
    "encoded_captions = video_captioner.predict(\n",
    "    vid_embeddings, beam_size=5).cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word ids to word tags\n",
    "captions = []\n",
    "for caption in encoded_captions:\n",
    "    captions.append(msrvtt_vocab.decode(\n",
    "        caption, clean=True, join=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('00:00:00.000', '00:00:02.080') a girl is singing on stage\n",
      "('00:00:02.080', '00:00:05.520') a girl is singing\n",
      "('00:00:05.520', '00:00:07.200') a girl is singing on stage\n",
      "('00:00:07.200', '00:00:10.280') a girl is singing on stage\n",
      "('00:00:10.280', '00:00:11.000') displaying on screen\n"
     ]
    }
   ],
   "source": [
    "for cap, t in zip(captions, scene_change_timecodes):\n",
    "    print (t, cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
